Mengying Yang 3033121912

2. a)
#download data
wget -O apricots http://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526&DataMartId=FAO&Format=csv&c=2,3,4,5,6,7&s=countryName:asc,elementCode:asc,year:desc

file apricots  #find file type

# unzip file
unzip apricots

#rename file 
mv UNdata_Export_20170907_235201079.csv apricots.csv

#look at data
less apricots.csv

#extract country and region (+)
grep + apricots.csv >>region.csv
grep -v + apricots.csv >>country.csv # -v stands for ignore the line contain +

#subset country-level data to the year 2005
grep "2005" country.csv>country2005.csv

#harvest
grep -i "Area Harvested" country2005.csv>country2005harvest.csv

#sort to find the five countries using the most land
	grep "2005" country.csv | sed 's/, / /g' | grep -i "area harvested" | sed 's/\"//g' | sort -r -n -t "," -k6 |head -5> rank.csv
#the first sed means change the ', ' to ' ' so that we can ignore the problems of sorting
#(which could lead China, mainland, have two delimiters and cannot sort the column six )
#the second sed means delete the " so that we can sort the year numerically
#then save only top five(head -5) countries' data to a rank.csv file

#automate your analysis and examine the top five countries for 1965, 1975, 1985, 1995, and 2005.
for ((yr=1965; yr<=2005; yr=yr+10)) 
	do  echo ${yr}>>rank1.csv; 
		grep "\"${yr}\"" country.csv | sed 's/, / /g' | grep -i "area harvested" | sed 's/\"//g' | sort -r -n -t "," -k6 |head -5>> rank1.csv; 
	done
#write a for loop to year each year from 1965, 1975, 1985, 1995, and 2005(difference is 10) and do the similar process like above for each year
#I also added a line of year before each year to seperate the result more clearly.
less rank1.csv
#through look at the result in rank1.csv, the rank has some changes through the year, but like Turkey always in the top five

2.b)

# I used helper.sh to save this function 

#code in helper.sh:
#create a function called helper
#!/bin/bash
helper() {
if [ $1 = "-h" ]     #detect if the input of user is -h
then
  echo "This is a helper function that downloads data and print the content out    #give useful help information if the user invokes the function as: “helper -h"
  usage:
  helper 526

  status code:
  0 success
  1 too much argument
  2 file download failure
  3 file unzip failure
  4 file empty data "
  return 0
fi               #First if end

if [ "$#" -ne 1 ]	#then detect if the user input too much argument 
then
  echo "Pass too much argument"
  return 1			#if so give a warning and return
fi

#if input is good for above conditions. download the file from the url which was inserted the itemcode input by the user
wget -q -O zipfile "http://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:$1&DataMartId=FAO&Format=csv&c=2,3,4,5,6,7&s=countryName:asc,elementCode:asc,year:desc"
if [ $? -ne 0 ]				#if wget return error code is not 0, give a error message and return 
then
  echo "File downloaded fail"
  return 2
fi

unzip -o -p zipfile > unzip.csv		#unzip the file if download successfully
if [ $? -ne 0 ]				#detect if unzip successfully
then
  echo "File unzip failed"
  return 3
fi

if [ $(wc -l unzip.csv | cut -d' ' -f1) -le 2 ]    # detect if the download file is smaller or equal to 2 line. If it is, give a warning that the data may empty and return
then 
  echo "The data you download may empty, change item code maybe"
  return 4
fi

while IFS= read -r line 		#if download successfully, read the file and print them out
do
  echo $line
done < unzip.csv

return 0
}


3.

#download the url and save it as index.html
wget http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/

#grep the lines contain txt and cut it to get the txt-files name. Then save them to the index.txt 
grep "txt" index.html | cut -d "=" -f5 | cut -d "\"" -f2 >index.txt

# write a for loop to download each file in the index.txt list 

for file in $(cat index.txt)
 
	do
 wget http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/$file;   # added the txt file name to the end of the url so that wget know which one we want to download 
	done



